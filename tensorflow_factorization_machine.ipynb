{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# modeling\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "from tqdm import tqdm\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "\n",
    "# other\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "cols_def = {\n",
    "    'answered_correctly' : 'con',\n",
    "    'timestamp'          : 'cat',\n",
    "#     'user_id'            : 'cat',\n",
    "    'content_id'         : 'cat',\n",
    "#     'part'               : 'cat',\n",
    "#     'content_mean'       : 'con',\n",
    "#     'part_mean'          : 'con',\n",
    "#     'kmeans_label'       : 'cat',\n",
    "#     'kmeans_mean'        : 'con',\n",
    "}\n",
    "\n",
    "mapping_dict = None\n",
    "with open('mapping_dict.pkl', 'rb') as f:\n",
    "    mapping_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(tf.keras.Model):\n",
    "    def __init__(self, k=4):\n",
    "        super(FM, self).__init__()\n",
    "        self.k = k\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, p =  input_shape\n",
    "\n",
    "        self.w0 = tf.Variable(tf.zeros([1]))\n",
    "        self.w  = tf.Variable(tf.zeros([p]))\n",
    "        self.V  = tf.Variable(tf.random.normal(shape=(p, self.k), stddev=0.01))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        linear_terms = tf.reduce_sum(tf.math.multiply(self.w, inputs), axis=1)\n",
    "\n",
    "        interactions = 0.5 * tf.reduce_sum(\n",
    "            tf.math.pow(tf.matmul(inputs, self.V), 2)\n",
    "            - tf.matmul(tf.math.pow(inputs, 2), tf.math.pow(self.V, 2)),\n",
    "            axis=1,\n",
    "            keepdims=False)\n",
    "\n",
    "        y_hat = tf.sigmoid(self.w0 + linear_terms + interactions)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', nrows=10000)\n",
    "df = df.loc[df.content_type_id==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "train, test = sklearn.model_selection.train_test_split(df, test_size=0.2)\n",
    "train, val = sklearn.model_selection.train_test_split(train, test_size=0.1)\n",
    "\n",
    "train = train[list(cols_def)].reset_index(drop=True,)\n",
    "test  = test[list(cols_def)].reset_index(drop=True,)\n",
    "val   = val[list(cols_def)].reset_index(drop=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.answered_correctly.values\n",
    "y_test = test.answered_correctly.values\n",
    "y_val = val.answered_correctly.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = []\n",
    "values = []\n",
    "\n",
    "for row in train[train.columns[1:]].reset_index(drop=True).reset_index().to_dict('records'):\n",
    "    for c in list(row):\n",
    "        if c=='index':\n",
    "            continue\n",
    "\n",
    "        if c not in cols_def:\n",
    "            continue\n",
    "            \n",
    "        if cols_def[c] == 'cat':\n",
    "            if row[c] not in mapping_dict[c]['features']:\n",
    "                continue\n",
    "\n",
    "            ix = [row['index'], mapping_dict[c]['features'][row[c]]]\n",
    "            v = 1\n",
    "        else:\n",
    "            ix = [row['index'], mapping_dict[c]['features']]\n",
    "            v = row[c]\n",
    "\n",
    "        indicies.append(ix)\n",
    "        values.append(v)\n",
    "\n",
    "X = tf.sparse.SparseTensor(indices=indicies, values=np.array(values, dtype=np.float32), dense_shape=(len(y_train),409562))\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X, np.reshape(y_train, (len(y_train))))).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = []\n",
    "values = []\n",
    "\n",
    "for row in test[test.columns[1:]].reset_index(drop=True).reset_index().to_dict('records'):\n",
    "    for c in list(row):\n",
    "        if c=='index':\n",
    "            continue\n",
    "\n",
    "        if c not in cols_def:\n",
    "            continue\n",
    "            \n",
    "        if cols_def[c] == 'cat':\n",
    "            if row[c] not in mapping_dict[c]['features']:\n",
    "                continue\n",
    "\n",
    "            ix = [row['index'], mapping_dict[c]['features'][row[c]]]\n",
    "            v = 1\n",
    "        else:\n",
    "            ix = [row['index'], mapping_dict[c]['features']]\n",
    "            v = row[c]\n",
    "\n",
    "        indicies.append(ix)\n",
    "        values.append(v)\n",
    "        \n",
    "_X = tf.sparse.SparseTensor(indices=indicies, values=np.array(values, dtype=np.float32), dense_shape=(len(y_test),409562))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((_X, np.reshape(y_test, (len(y_test))))).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = []\n",
    "values = []\n",
    "\n",
    "for row in val[val.columns[1:]].reset_index(drop=True).reset_index().to_dict('records'):\n",
    "    for c in list(row):\n",
    "        if c=='index':\n",
    "            continue\n",
    "\n",
    "        if c not in cols_def:\n",
    "            continue\n",
    "            \n",
    "        if cols_def[c] == 'cat':\n",
    "            if row[c] not in mapping_dict[c]['features']:\n",
    "                continue\n",
    "\n",
    "            ix = [row['index'], mapping_dict[c]['features'][row[c]]]\n",
    "            v = 1\n",
    "        else:\n",
    "            ix = [row['index'], mapping_dict[c]['features']]\n",
    "            v = row[c]\n",
    "\n",
    "        indicies.append(ix)\n",
    "        values.append(v)\n",
    "        \n",
    "_X = tf.sparse.SparseTensor(indices=indicies, values=np.array(values, dtype=np.float32), dense_shape=(len(y_val),409562))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((_X, np.reshape(y_val, (len(y_val))))).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "train_auc = tf.keras.metrics.AUC(name='train_auc')\n",
    "test_auc = tf.keras.metrics.AUC(name='test_auc')\n",
    "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.2)\n",
    "\n",
    "_lambda = tf.Variable(0.0002, name='lambda')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(inputs)\n",
    "        \n",
    "        loss = tf.keras.losses.binary_crossentropy(from_logits=True,\n",
    "                                                        y_true=targets,\n",
    "                                                        y_pred=y_pred)\n",
    "\n",
    "        l2_reg = _lambda*(tf.nn.l2_loss(model.w) + tf.nn.l2_loss(model.V))\n",
    "        loss = loss + l2_reg\n",
    "        \n",
    "    grads = tape.gradient(target=loss, sources=model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    train_auc.update_state(targets, y_pred)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, inputs, targets):\n",
    "\n",
    "    y_pred = model(inputs, training=False)\n",
    "    \n",
    "    loss = tf.keras.losses.binary_crossentropy(from_logits=True,\n",
    "                                               y_true=targets, \n",
    "                                               y_pred=y_pred)\n",
    "\n",
    "    l2_reg = _lambda*(tf.nn.l2_loss(model.w) + tf.nn.l2_loss(model.V))\n",
    "    loss = loss + l2_reg\n",
    "\n",
    "    test_auc.update_state(targets, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "model = FM(4)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'Starting Epoch: {epoch+1}')\n",
    "        # 다음 epoch의 평가지표 초기화 \n",
    "        train_auc.reset_states()\n",
    "        test_auc.reset_states()\n",
    "\n",
    "\n",
    "        for step, (sparse_batch, labels) in enumerate(train_ds):\n",
    "            sparse_batch = tf.sparse.to_dense(sparse_batch)\n",
    "            sparse_batch = tf.where(tf.math.is_nan(sparse_batch), tf.zeros_like(sparse_batch), sparse_batch)\n",
    "#             sparse_batch = tf.divide(\n",
    "#                 tf.subtract(sparse_batch, tf.reduce_min(sparse_batch, axis=0, keepdims=True)),\n",
    "#                 tf.subtract(tf.reduce_max(sparse_batch, axis=0, keepdims=True), tf.reduce_min(sparse_batch, axis=0, keepdims=True))\n",
    "#             )\n",
    "            loss = train_step(model, optimizer, sparse_batch, labels)\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(f'Training Loss (for one batch) at step {step}: {loss:.5f}')\n",
    "                print(f'Seen so far: {((step+1)*BATCH_SIZE)} samples')\n",
    "\n",
    "\n",
    "        print(f'Training AUC over epoch: {train_auc.result():.5f}')\n",
    "\n",
    "        for step, (sparse_batch, labels) in enumerate(val_ds):\n",
    "            sparse_batch = tf.sparse.to_dense(sparse_batch)\n",
    "            sparse_batch = tf.where(tf.math.is_nan(sparse_batch), tf.zeros_like(sparse_batch), sparse_batch)\n",
    "#             sparse_batch = tf.divide(\n",
    "#                 tf.subtract(sparse_batch, tf.reduce_min(sparse_batch, axis=0, keepdims=True)),\n",
    "#                 tf.subtract(tf.reduce_max(sparse_batch, axis=0, keepdims=True), tf.reduce_min(sparse_batch, axis=0, keepdims=True))\n",
    "#             )\n",
    "            t_loss = test_step(model, sparse_batch, labels)\n",
    "\n",
    "        print(f'Test loss: {t_loss:.5f}')\n",
    "        print(f'Test AUC: {test_auc.result():.5f}')\n",
    "        print('==========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for a,b in test_ds:\n",
    "        a = tf.sparse.to_dense(a)\n",
    "        a = tf.where(tf.math.is_nan(a), tf.zeros_like(a), a)\n",
    "        print(np.mean(model.predict(a)))\n",
    "\n",
    "        y_pred += list(model.predict(a))\n",
    "        y_true += list(b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_jitter(arr, tol):\n",
    "    stdev = tol*(max(arr)-min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "\n",
    "plt.scatter(y_pred, rand_jitter(y_true, 0.1), alpha=0.2)\n",
    "plt.xlabel('Correct Probability')\n",
    "plt.yticks([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_true,  y_pred)\n",
    "auc = sklearn.metrics.roc_auc_score(y_true,  y_pred)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
